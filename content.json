{"pages":[{"title":"About","date":"2009-04-22T19:24:48.000Z","path":"about/index.html","text":""},{"title":"Tags","date":"2009-04-22T19:24:48.000Z","path":"tags/index.html","text":""},{"title":"Categories","date":"2009-04-22T19:24:48.000Z","path":"categories/index.html","text":""}],"posts":[{"title":"spark","date":"2023-06-14T05:40:30.000Z","path":"wiki/spark/","text":"（1）安装Anaconda 上传安装包 sh ./Anaconda3-2021.05-Linux-x86_64.sh 出现（base)即为安装成功 （2）创建虚拟环境 conda create -n pyspark python=3.8 conda activate pyspark pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn /simple （1）修改环境变量配置Spark由如下5个环境变量需要设置 SPARK_HOME: 表示Spark安装路径在哪里 PYSPARK_PYTHON: 表示Spark想运行Python程序, 那么去哪里找python执行器。 JAVA_HOME: 告知Spark Java在哪里 HADOOP_CONF_DIR: 告知Spark Hadoop的配置文件在哪里 HADOOP_HOME: 告知Spark Hadoop安装在哪里 这5个环境变量 都需要配置在: /etc/profile中！ （4）解压 解压下载的Spark安装包 tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /export/server/ 设置软连接 ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark （2）测试 bin/pyspark 在这个环境可以运行spark代码，如图： sc.parallelize([1,2,3,4,5]).map(lambda x: x + 1).collect() Spark StandAlone**环境部署** 在所有机器安装Python(Anaconda)，并在所有机器配置环境变量。 （1）配置配置文件 进入到spark的配置文件目录中, cd $SPARK_HOME/conf` 配置workers文件vi workers # 改名, 去掉后面的.template后缀 mv workers.template workers # 编辑worker文件7 vim workers # 将里面的localhost删除, 追加 node1 node2 node3 到workers文件内 （2）配置spark-env.sh文件 # 1. 改名 mv spark-env.sh.template spark-env.sh # 2. 编辑spark-env.sh, 在底部追加如下内容 ## 设置JAVA安装目录 JAVA_HOME=/export/server/jdk ## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop YARN_CONF_DIR=/export/server/hadoop/etc/hadoop ## 指定spark老大Master的IP和提交任务的通信端口 # 告知Spark的master运行在哪个机器上 export SPARK_MASTER_HOST=node1 # 告知sparkmaster的通讯端口 export SPARK_MASTER_PORT=7077 # 告知spark master的 webui端口 SPARK_MASTER_WEBUI_PORT=8080 # worker cpu可用核数 SPARK_WORKER_CORES=1 # worker可用内存 SPARK_WORKER_MEMORY=1g # worker的工作通讯地址 SPARK_WORKER_PORT=7078 # worker的 webui地址 SPARK_WORKER_WEBUI_PORT=8081 ## 设置历史服务器 # 配置的意思是 将spark程序运行的历史日志 存到hdfs的/sparklog文件夹中 SPARK_HISTORY_OPTS=”-Dspark.history.fs.logDirectory=hdfs://node1:8020/sparklog/ -Dspark.history.fs.cleaner.enabled=true” 在HDFS上创建程序运行历史记录存放的文件夹: hadoop fs -mkdir /sparklog hadoop fs -chmod 777 /sparklog （3）配置spark-defaults.conf文件 # 1. 改名 mv spark-defaults.conf.template spark-defaults.conf # 2. 修改内容, 追加如下内容 # 开启spark的日期记录功能 spark.eventLog.enabled true # 设置spark日志记录的路径 spark.eventLog.dir hdfs://node1:8020/sparklog/ # 设置spark日志是否启动压缩 spark.eventLog.compress true （4）配置log4j.properties 文件 [可选配置] mv log4j.properties.template log4j.properties 注意：将Spark安装文件夹 分发到其它的服务器 1）scp -r spark-3.1.2-bin-hadoop3.2 node2:/export/server/ 2）scp -r spark-3.1.2-bin-hadoop3.2 node3:/export/server/ 3）在node2和node3上 给spark安装目录增加软链接 ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark （5）启动历史服务器 sbin/start-history-server.sh （6）启动Spark的Master和Worker进程 sbin/start-all.sh sbin/start-master.sh sbin/start-worker.sh sbin/stop-all.sh sbin/stop-master.sh sbin/stop-worker.sh 查看Master的WEB UI 连接到StandAlone集群 参考资料","tags":[],"categories":[]},{"title":"hive","date":"2023-06-14T04:50:15.000Z","path":"wiki/hive/","text":"1**）Mysql安装** 1）卸载Centos7自带的mariadb 如果出现了mariadb-libs-5.5.64-1.el7.x86_64，输入rpm -e mariadb- libs-5.5.64-1.el7.x86_64 –nodeps,在输入rpm -qa|grep mariadb，即可 2）安装mysql 新建文件夹：mkdir /export/server/mysql 上传mysql-5.7.29-1.el7.x86_64.rpm-bundle.tar到上述文件夹下，解压tar xvf mysql-5.7.29-1.el7.x86_64.rpm-bundle.tar 3）执行安装 yum -y install libaio 4）mysql初始化设置 初始化：mysqld –initialize 更改所属组：chown mysql:mysql /var/lib/mysql -R 启动mysql：systemctl start mysqld.service 查看临时生成的root密码：cat /var/log/mysqld.log 5）修改root密码 授权远程访问 设置开机自启动 修改root密码 设置为hadoop 授权 use mysql; GRANT ALL PRIVILEGES ON . TO ‘root‘@’%’ IDENTIFIED BY ‘hadoop’ WITH GRANT OPTION; FLUSH PRIVILEGES; mysql的启动和关闭 状态查看 （这几个命令必须记住） systemctl stop mysqld systemctl status mysqld systemctl start mysqld 设置开机自动启动：systemctl enable mysqld 查看是否设置自动启动成功 （**2）Hive**的安装 1）上传安装包 解压 tar zxvf apache-hive-3.1.2-bin.tar.gz ln -s apache-hive-3.1.2-bin hive 2）解决Hive与Hadoop之间guava版本差异 cd /export/server/hive/ rm -rf lib/guava-19.0.jar cp /export/server/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar ./lib/ 3）修改配置文件 hive-env.sh cd /export/server/hive/conf mv hive-env.sh.template hive-env.sh vim hive-env.sh export HADOOP_HOME=/export/server/hadoop export HIVE_CONF_DIR=/export/server/hive/conf export HIVE_AUX_JARS_PATH=/export/server/hive/lib hive-site.xml vim hive-site.xml ​ javax.jdo.option.ConnectionURL​ jdbc:mysql://node1:3306/hive3?createDatabaseIfNotExist=true&amp;useSSL=false&amp;useUnicode=true&amp;characterEncoding=UTF-8​ javax.jdo.option.ConnectionDriverName​ com.mysql.jdbc.Driver​ javax.jdo.option.ConnectionUserName​ root​ javax.jdo.option.ConnectionPassword​ hadoop hive.server2.thrift.bind.host node1 hive.metastore.uris thrift://node1:9083 hive.metastore.event.db.notification.api.auth false 4）上传mysql jdbc驱动到hive安装包lib下 mysql-connector-java-5.1.32.jar 5）初始化元数据 cd /export/server/hive/ bin/schematool -initSchema -dbType mysql -verbos 初始化成功之后会在MySQL中创建74张表 6）在hdfs创建hive存储目录（如存在则不用操作） hadoop fs -mkdir /tmp hadoop fs -mkdir -p /user/hive/warehouse hadoop fs -chmod g+w /tmp hadoop fs -chmod g+w /user/hive/warehouse 7）启动hive （**3）启动metastore**服务 前台启动 关闭ctrl+c /export/server/hive/bin/hive –service metastore 前台启动开启debug日志 /export/server/hive/bin/hive –service metastore –hiveconf hive.root.logger=DEBUG,console 后台启动 进程挂起 关闭使用jps+ kill -9 nohup /export/server/hive/bin/hive –service metastore &amp; （**4）启动hiveserver2**服务 nohup /export/server/hive/bin/hive –service hiveserver2 &amp; beeline客户端连接 拷贝node1安装包到beeline客户端机器上（node3） scp -r /export/server/apache-hive-3.1.2-bin/ root@node3:/export/server/ （**5）hive**注释信息中文乱码解决 以下sql语句均在mysql数据库中执行 use hivenode2; show tables; alter table hivenode2.COLUMNS_V2 modify column COMMENT varchar(256) character set utf8; alter table hivenode2.TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8; alter table hivenode2.PARTITION_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8 ; alter table hivenode2.PARTITION_KEYS modify column PKEY_COMMENT varchar(4000) character set utf8; alter table hivenode2.INDEX_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8; 参考资料","tags":[],"categories":[]},{"title":"git","date":"2023-06-14T04:47:29.000Z","path":"wiki/git/","text":"（1）Git下载 核心程序 （2）可视化客户端 中文语言包 （3）初始化仓库 （4）添加文件，提交文件至本地仓库 （5）本地删除与恢复 文件选中删除，可用以下方式还原 （6）创建分支 （7）分支的查看切换 （8）标签的创建 （9）切换与删除 通过右键选中删除 远程仓库 （1）码云账号注册 填写邮箱发送验证码,然后可以注册账号,主页如下 （2）创建远程仓库 （3）把本地代码推送到远端 生成公钥私钥 ssh-keygen -t rsa 一直回车 会默认用户目录 .ssh 目录生成一个默认的id_rsa文件 和id_rsa.pub 密钥配置 参考资料","tags":[],"categories":[]},{"title":"Hello World","date":"2023-06-08T02:21:40.060Z","path":"wiki/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[],"categories":[]}],"categories":[],"tags":[]}