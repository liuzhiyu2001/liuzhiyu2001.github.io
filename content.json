{"pages":[{"title":"Categories","date":"2009-04-22T19:24:48.000Z","path":"categories/index.html","text":""},{"title":"Tags","date":"2009-04-22T19:24:48.000Z","path":"tags/index.html","text":""},{"title":"About","date":"2009-04-22T19:24:48.000Z","path":"about/index.html","text":""}],"posts":[{"title":"Hello World","date":"2023-06-14T13:14:19.290Z","path":"wiki/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[],"categories":[]},{"title":"pyspark基础编码环境","date":"2023-06-07T15:19:34.000Z","path":"wiki/pyspark基础编码环境/","text":"（一）、pyspark环境配置安装。 PySpark是Spark官方提供的一个Python类库，内置了Spark API，可以通过PySpark类库来编写Spark程序，并提交到Spark集群中运行。前情提示： （1）将课程资料中提供的的hadoop-3.3.0文件，复制到某个盘符下（中文的且无空格的）。 （2）将文件夹内bin内的Hadoop.dll复制到C:\\Windows\\Systmctl32里面去。 （3）在系统环境变量中配置HADOOP_HOME，指向hadoop-3.3.0文件夹的路径。 （二）本机PySpark环境配置 在前面部署Spark的时候，已经在Linux系统上部署了acaconda的Python环境，详见Spark的Stand Alone模式部署章节。故本次在Windows上安装anaconda，并配置PySpark库。具体安装步骤如下： （1）在课程资料中选择anaconda应用程序双击安装。 （2）一直选择Next，进行安装。 注意：选择第一个，将anaconda添加至我的环境变量中！ （1）安装结束后会出现anaconda3文件夹。打开Anaconda Prompt(anaconda),会出现base，即为安装成功。 （4）配置国内源，加速网络下载。 1、在Anaconda Prompt(anaconda)中执行conda config –set show_channel _urls yes。 2、将如下内容替换到C:\\Users\\用户名.condarc文件中。 channels: - defaults show_channel_urls: true default_channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2 custom_channels: conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud （5）创建虚拟环境 1、创建虚拟环境 pyspark, 基于Python 3.8 conda create -n pyspark python=3.8 2、切换到虚拟环境内 conda activate pyspark 3、在虚拟环境内安装包 pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/ simple 安装成功示例： （三）PyCharm中配置Python解释器 （1）配置本地解释器：创建Python项目，选择conda虚拟环境PySpark中的Python.exe解释器。 （2）配置远程SSH Linux解释器 1、远程SSH python pyspark环境 2、添加新的远程连接 3、设置虚拟的python环境路径 （四）WordCount应用实战 可以选择在本地的PySpark环境中执行spark代码，也可以选择在虚拟机环境PySpark中执行。选择本地的就是使用conda环境，应用其中的PySpark环境执行，来读取本地文件，完成单词计数的实例。选择远程虚拟机中的PySpark环境，需要SSH连接到服务器（这里需要安装Pycharm专业版），注意：无论是选择那种方案，都是在PyCharm软件中去执行，完成上述过程。 （1）WordCount代码本地执行 准备pyspark代码以及本地文件words.txt，在PyCharm中执行。 # coding:utf8 from pyspark import SparkConf, SparkContext # import os import os os.environ[‘PYSPARK_PYTHON’]=’D:\\anaconda3\\envs\\pyspark\\python.exe’ os.environ [‘JAVA_HOME’] = ‘D:\\Java\\jdk1.8.0_241’ #os.environ[‘PYSPARK_PYTHON’]=’/export/server/anaconda3/envs/pyspark/bin/python3.8’ if name == ‘main‘: conf = SparkConf().setAppName(“WordCountHelloWorld”).setMaster(“local[*]”) # 通过SparkConf对象构建SparkContext对象 sc = SparkContext(conf=conf) # 需求 : wordcount单词计数, 读取HDFS上的words.txt文件, 对其内部的单词统计出现 的数量 # 读取文件 #file_rdd = sc.textFile(“hdfs://node1:8020/input/words.txt”) #file_rdd = sc.textFile(“file:///tmp/pycharm_project_621/data/words.txt”) file_rdd = sc.textFile(“D:\\数据挖掘与分析实验报告合集\\pyspark\\data\\input\\words.txt”) # 将单词进行切割, 得到一个存储全部单词的集合对象 words_rdd = file_rdd.flatMap(lambda line: line.split(“ “)) # 将单词转换为元组对象, key是单词, value是数字1 words_with_one_rdd = words_rdd.map(lambda x: (x, 1)) # 将元组的value 按照key来分组, 对所有的value执行聚合操作(相加) result_rdd = words_with_one_rdd.reduceByKey(lambda a, b: a + b) # 通过collect方法收集RDD的数据打印输出结果 print(result_rdd.collect()) 运行结果截图： （2）WordCount代码远程服务器上执行。 通过SSH连接到远程服务器上，详见上述操作。 完成与服务器连接后，会在服务器中的/tmp文件夹下新建了pycharm_project_xxx文件夹用于放置本地的同步代码。 （3）读取HDFS上的文件 1、将读取文件路径改为hdfs上的/input/words.txt 2、在hdfs上新建/input文件夹，使用命令hadoop fs -mkdir /input 3、上传words.txt到hdfs中，使用命令hadoop fs -put words.txt /input/ 4、在pycharm中执行spark代码 （五）spark-submit作业提交 （1）local本地模式 首先将helloword.py程序放到/root/目录下，使用命令bin/spark-submit –master local[*] /root/helloworld.py完成提交作业。 （2）spark on yarn模式 使用命令bin/spark-submit –master yarn /root/helloworld.py完成提交作业。 （3）使用历史服务器查看任务执行情况 node1:18080","tags":[{"name":"搭建","slug":"搭建","permalink":"http://example.com/tags/%E6%90%AD%E5%BB%BA/"},{"name":"spark","slug":"spark","permalink":"http://example.com/tags/spark/"}],"categories":[{"name":"工具","slug":"工具","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7/"}]},{"title":"YARN模式","date":"2023-06-07T15:15:42.000Z","path":"wiki/YARN模式/","text":"（1）Client模式中driver运行在客户端，在客户端显示输出结果，但是在spark历史服务器不显示logs信息。 （2）Cluster模式中driver运行在YARN容器内部，和ApplicationMaster在同一个容器内，在客户端不显示输出结果，所以在spark历史服务器中显示logs的信息。 （3）client模式测试 bin/spark-submit –master yarn –deploy-mode client –driver-memory 512m ${SPARK_HOME}/examples/src/main/python/pi.py 10 （4） cluster模式测试 bin/spark-submit –master yarn –deploy-mode cluster –driver-memory 512m --conf “spark.pyspark.driver.python=/export/server/anaconda3 /bin/python3” --conf “spark.pyspark.python=/export/server/anaconda3 /bin/python3” ${SPARK_HOME}/examples/src/main/python/pi.py 10","tags":[{"name":"搭建","slug":"搭建","permalink":"http://example.com/tags/%E6%90%AD%E5%BB%BA/"},{"name":"spark","slug":"spark","permalink":"http://example.com/tags/spark/"}],"categories":[{"name":"工具","slug":"工具","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7/"}]},{"title":"Spark-HA环境部署","date":"2023-06-07T15:07:49.000Z","path":"wiki/Spark-HA环境部署/","text":"（1）首先进入spark-env.sh中，vim /export/server/spark/conf/spark -env.sh （2）在spark-env.sh配置文件中删除 export SPARK_MASTER_ HOST=node1 （目的是不然机器知道固定的master是谁，不然无法进行master切换） （3）在spark-env.sh配置文件中增加以下内容： SPARK_DAEMON_JAVA_OPTS=”-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=node1:2181,node2:2181,node3:2181 -Dspark.deploy.zookeeper.dir=/spark-ha” # spark.deploy.recoveryMode 指定HA模式 基于Zookeeper实现 # 指定Zookeeper的连接地址 # 指定在Zookeeper中注册临时节点的路径 （4）将spark-env.sh配置文件分发给node2、node3。 scp -r /export/server/spark/conf/spark-env.sh node2:/export/server/spark /conf/ scp -r /export/server/spark/conf/spark-env.sh node3:/export/server/spark /conf/ （5）启动StandAlone集群、zookeeper集群： 1）在node1上：sbin/start-all.sh 2）在node2上：sbin/start-master.sh （目的是：备用master，当kill掉node1的master后，程序依然能进行） （5）查看node1、node2的WEB UI （如果8080端口被占用了，可以顺延到8081、8082端口， 其中node1上的master是alive的，node2上的是standby） node1:8080–&gt;8081 node2:8080–&gt;8081\\8082 （6）Master主备切换，在/export/server/spark路径下提交一个任务到当前alive master上: bin/spark-submit –master spark://node1:7077 \\ /export/server/spark/examples/src/main/python/pi.py 1000 （在提交成功后, 将alive master直接kill掉，系统不会中断，仍然能正常运行结果） （7）查看Master的WEB UI，只有node2是alive的，证明master切换成功","tags":[{"name":"搭建","slug":"搭建","permalink":"http://example.com/tags/%E6%90%AD%E5%BB%BA/"},{"name":"spark","slug":"spark","permalink":"http://example.com/tags/spark/"}],"categories":[{"name":"工具","slug":"工具","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7/"}]},{"title":"Spark-StandAlone环境部署","date":"2023-06-07T14:51:41.000Z","path":"wiki/Spark-StandAlone环境部署/","text":"（一）、集群规划：选择三台机器分别为node1、node2、node3来组成集群环境。 其中node1上安装master和worker进程；node2上安装worker进程；node3上安装worker进程。 （二）、anaconda on linux安装过程： （1）前提：在linux服务器node1、node2、node3上都安装python(anaconda)。并安装pyspark虚拟环境。具体安装步骤如下。 1、在/export/server/目录下上传anaconda的安装包Anaconda3-2021. 05-Linux-x86_64.sh。 2、安装anaconda 使用命令：sh ./Anaconda3-2021.05-Linux-x86_64.sh 3、安装完毕之后若没有出现base环境，进行如下配置。在/root/.condarc添加国内源 安装完毕后，关闭服务器重新启动，出现base环境即安装成功。 （2）在anaconda中，安装pyspark虚拟环境。 1、基于python3.8安装pyspark环境。 2、切换到pyspark中，并安装所需要的安装包。 注：在node1、node2、node3三台服务器上都完成配置！ （三）、StandAlone模式部署 （1）安装spark压缩文件。 1、进入到/export/server/中上传并解压spark-3.2.0-bin-hadoop3.2.tgz。并设置软链接，命令为ln-s/export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark。 （2）在/export/server/spark/conf，配置文件。 1、首先在配置workers文件。mv workers.template workers；vim workers； 2.配置spark-env.sh文件。mv spark-env.sh.template spark-env.sh； Vim spark-env.sh，添加如下内容。 ## 设置JAVA安装目录 JAVA_HOME=/export/server/jdk ## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop YARN_CONF_DIR=/export/server/hadoop/etc/hadoop ## 指定spark老大Master的IP和提交任务的通信端口 # 告知Spark的master运行在哪个机器上 export SPARK_MASTER_HOST=node1 # 告知sparkmaster的通讯端口 export SPARK_MASTER_PORT=7077 # 告知spark master的 webui端口 SPARK_MASTER_WEBUI_PORT=8080 # worker cpu可用核数 SPARK_WORKER_CORES=1 # worker可用内存 SPARK_WORKER_MEMORY=1g # worker的工作通讯地址 SPARK_WORKER_PORT=7078 # worker的 webui地址 SPARK_WORKER_WEBUI_PORT=8081 ## 设置历史服务器 # 配置的意思是 将spark程序运行的历史日志 存到hdfs的/sparklog文件夹中 SPARK_HISTORY_OPTS=”-Dspark.history.fs.logDirectory=hdfs://node1:8020/sparklog/ -Dspark.history.fs.cleaner.enabled=true” 3、在HDFS上创建程序运行历史记录存放的文件夹。 hadoop fs -mkdir /sparklog；hadoop fs -chmod 777 /sparklog 4、配置spark-defaults.conf文件。mv spark-defaults.conf.template spark-defaults.conf；vim spark-defaults.conf，添加如下内容。 5、配置log4j.properties 文件[可选配置]。mv log4j.properties.template log4j.properties；修改配置，设置级别为WARN 只输出警告和错误日志。 （四）、将spark分发到node2和node3服务器上。注意同时要设置软链接。 scp -r spark-3.1.2-bin-hadoop3.2 node2:/export/server/ scp -r spark-3.1.2-bin-hadoop3.2 node3:/export/server/ ln -s /export/server/spark-3.1.2-bin-hadoop3.2 /export/server/spark 注意：配置/etc/profile，JAVA_HOME；SPARK_HOME；PYSPARK_PYTHON都指向正确的目录。 （五）、启动历史服务器，启动Spark的Master和Worker进程 （1）启动历史服务器：sbin/start-history-server.sh （2）启动全部的master和worker：sbin/start-all.Sh （六）、查看Master的WEB UI 在浏览器中输入node1:8080 （七）、连接到StandAlone集群 （1）通过master来连接到StandAlone集群。 bin/pyspark –master spark://node1:7077 （2）使用spark-shell连接StandAlone集群。 bin/spark-shell –master spark://node1:7077，进行测试。 （3）使用spark-submit(PI)提交任务到集群上执行。bin/spark-submit –master spark://node1:7077/export/server/spark/examples/src/main/Pytho n/pi.py 10 查看历史服务器：在浏览器中输入node1：18080","tags":[{"name":"搭建","slug":"搭建","permalink":"http://example.com/tags/%E6%90%AD%E5%BB%BA/"},{"name":"spark","slug":"spark","permalink":"http://example.com/tags/spark/"}],"categories":[{"name":"工具","slug":"工具","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7/"}]},{"title":"Spark-local环境部署","date":"2023-06-07T14:35:48.000Z","path":"wiki/Spark-local环境部署/","text":"（1）安装Anaconda 上传安装包 sh ./Anaconda3-2021.05-Linux-x86_64.sh 出现（base)即为安装成功 （2）创建虚拟环境 conda create -n pyspark python=3.8 conda activate pyspark pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn /simple （1）修改环境变量配置Spark由如下5个环境变量需要设置 SPARK_HOME: 表示Spark安装路径在哪里 PYSPARK_PYTHON: 表示Spark想运行Python程序, 那么去哪里找python执行器。 JAVA_HOME: 告知Spark Java在哪里 HADOOP_CONF_DIR: 告知Spark Hadoop的配置文件在哪里 HADOOP_HOME: 告知Spark Hadoop安装在哪里 这5个环境变量 都需要配置在: /etc/profile中！ （4）解压 解压下载的Spark安装包 tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /export/server/ 设置软连接 ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark （2）测试 bin/pyspark 在这个环境可以运行spark代码，如图： sc.parallelize([1,2,3,4,5]).map(lambda x: x + 1).collect() \\Spark StandAlone环境部署** 在所有机器安装Python(Anaconda)，并在所有机器配置环境变量。 （1）配置配置文件 进入到spark的配置文件目录中, cd $SPARK_HOME/conf` 配置workers文件vi workers # 改名, 去掉后面的.template后缀 mv workers.template workers # 编辑worker文件 vim workers # 将里面的localhost删除, 追加 node1 node2 node3 到workers文件内 （2)配置spark-env.sh文件 # 1. 改名 mv spark-env.sh.template spark-env.sh # 2. 编辑spark-env.sh, 在底部追加如下内容 ## 设置JAVA安装目录 JAVA_HOME=/export/server/jdk ## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop YARN_CONF_DIR=/export/server/hadoop/etc/hadoop ## 指定spark老大Master的IP和提交任务的通信端口 # 告知Spark的master运行在哪个机器上 export SPARK_MASTER_HOST=node1 # 告知sparkmaster的通讯端口 export SPARK_MASTER_PORT=7077 # 告知spark master的 webui端口 SPARK_MASTER_WEBUI_PORT=8080 # worker cpu可用核数 SPARK_WORKER_CORES=1 # worker可用内存 SPARK_WORKER_MEMORY=1g # worker的工作通讯地址 SPARK_WORKER_PORT=7078 # worker的 webui地址 SPARK_WORKER_WEBUI_PORT=8081 ## 设置历史服务器 # 配置的意思是 将spark程序运行的历史日志 存到hdfs的/sparklog文件夹中 SPARK_HISTORY_OPTS=”-Dspark.history.fs.logDirectory=hdfs://node1:8020/sparklog/ -Dspark.history.fs.cleaner.enabled=true” 在HDFS上创建程序运行历史记录存放的文件夹: hadoop fs -mkdir /sparklog hadoop fs -chmod 777 /sparklog （3）配置spark-defaults.conf文件 # 1. 改名 mv spark-defaults.conf.template spark-defaults.conf # 2. 修改内容, 追加如下内容 # 开启spark的日期记录功能 spark.eventLog.enabled true # 设置spark日志记录的路径 spark.eventLog.dir hdfs://node1:8020/sparklog/ # 设置spark日志是否启动压缩 spark.eventLog.compress true （4)配置log4j.properties 文件 [可选配置] mv log4j.properties.template log4j.properties 注意：将Spark安装文件夹 分发到其它的服务器 1）scp -r spark-3.1.2-bin-hadoop3.2 node2:/export/server/ 2）scp -r spark-3.1.2-bin-hadoop3.2 node3:/export/server/ 3）在node2和node3上 给spark安装目录增加软链接 ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark （5）启动历史服务器 sbin/start-history-server.sh （6）启动Spark的Master和Worker进程 sbin/start-all.sh sbin/start-master.sh sbin/start-worker.sh sbin/stop-all.sh sbin/stop-master.sh sbin/stop-worker.sh 查看Master的WEB UI 连接到StandAlone集群","tags":[{"name":"搭建","slug":"搭建","permalink":"http://example.com/tags/%E6%90%AD%E5%BB%BA/"},{"name":"spark","slug":"spark","permalink":"http://example.com/tags/spark/"}],"categories":[{"name":"工具","slug":"工具","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7/"}]},{"title":"Docker安装","date":"2023-06-07T11:55:26.000Z","path":"wiki/Docker安装/","text":"（1）卸载（可选） 如果之前安装过旧版本的Docker，可以使用下面命令卸载： yum remove docker \\ ​ docker-client \\ ​ docker-client-latest \\ ​ docker-common \\ ​ docker-latest \\ ​ docker-latest-logrotate \\ ​ docker-logrotate \\ ​ docker-selinux \\ ​ docker-engine-selinux \\ ​ docker-engine \\ ​ docker-ce （2）yum源配置 1.备份配置文件 1）mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS -Base.repo.backup 2）wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun .com/repo/Centos-7.repo 3）wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/rep o/epel-7.repo 4）yum clean all 5）yum makecache 6）yum install -y bash-completion vim lrzsz wget expect net-tools nc nmap treedos2unix htop iftop iotop unzip telnet sl psmisc nethogs glances bc ntpdate openldap-devel \\安装docker** （1）受限需要虚拟机联网，安装yum工具 （2）配置网卡转发 1）docker必须安装在centos7平台，内核版本不低于3.10在centos平台运行docker可能会遇见些告警信息，修改内核配置参数，打开内核转发功能 写入 2）重新加载内核参数 modprobe br_netfilter sysctl -p /etc/sysctl.d/docker.conf （3）利用yum进行docker安装 提前配置好yum仓库 1）阿里云自带仓库 curl -o /etc/yum.repos.d/Centos-7.repo http://mirrors.aliyun.com/repo/ Centos-7.repo 2）阿里云提供的docker专属repo仓库 curl-o/etc/yum.repos.d/docker-ce.repohttp://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 3）更新yum缓存 yum clean all &amp;&amp; yum makecache 4）查看源中可用版本 yum list docker-ce –showduplicates | sort -r 5）yum安装 yum install docker-ce -y docker -v 卸载 yum remove -y docker-ce-xxx （4）配置镜像加速器 用于加速镜像文件下载,选用阿里云镜像站 1）mkdir -p /etc/docker touch /etc/docker/daemon.json 2）进入文件vim /etc/docker/daemon.json编写以下内容： { “registry-mirrors” : [ “https://8xpk5wnt.mirror.aliyuncs.com&quot; ] } （5）启动docker 1）关闭防火墙：systemctl stop firewalld 2）禁止开机启动防火墙：systemctl disable firewalld 3）查看防火墙状态：systemctl status firewalld 通过命令启动docker： systemctl start docker 启动docker服务 systemctl stop docker 停止docker服务 systemctl restart docker 重启docker服务 docker配置文件重新加载：systemctl daemon-reload 设置开启自启动：systemctl enable docker （6）查看docker信息：docker info （7）显示当前正在运行的容器：docker ps （8）docker镜像：docker images （9）docker版本：docker version docker-client which docker docker daemon，运行在docker host上，负责创建、运行、监控容器、构建、存储镜像 ps aux |grep docker containerd ps aux|grep containerd systemctl status containerd \\docker的基本操作** 启动第一个docker容器 Nginx web服务器，运行一个80端口的网站 在宿主机上，运行Nginx 开启服务器 2.在服务器上安装好运行nginx所需的依赖关系 3.安装nginx yum install nginx -y 4.修改nginx配置文件 5.启动nginx 6.客户端去访问nginx （1）查看本地的docker镜像有哪些：docker image ls 或 docker images （2）可选择删除旧版本：docker rmi 镜像id （3）搜索一下远程仓库中的镜像文件是否存在：docker search nginx （4）docker pull nginx （5）再次查看镜像：docker images （6）运行镜像，运行出具体内容，在容器中就跑着一个nginx服务，docker run 参数 镜像的名字/id -d 后台运行容器 -p 80:80 端口映射，宿主机端口：容器内端口，访问宿主机的80端口，也就访问到容器中的80端口，会返回一个容器的id docker run -d -p 80:80 nginx （7）查看容器是否在运行：docker ps （8）访问网站192.168.88.163:80 （9）停止容器：docker stop 容器id （10）恢复容器：docker start 容器id \\docker镜像原理** （1）查看发行版： cat /etc/redhat-release （2）查看内核：uname -r （3）利用docker获取不同的发行版镜像（例如centos：7.8.2003）：docker pull centos:7.8.2003 （4）确认当前宿主机的发行版：cat /etc/redhat-release （5）运行centos:7.8.2003发行版本 运行容器，且进入容器内部 参数解释，-i 交互式命令操作 -t 开启一个终端 bash 进入容器后执行的命令 docker run -it afb6fca791e0 bash （6）退出容器空间：exit \\获取镜像** （1）docker search 镜像名:tag tag就是具体的标签版本：docker search centos （2）查看docker镜像的存储路径：docker info |grep Root （3）具体位置：ls /var/lib/docker/image/overlay2/imagedb/content/sha256 （4）使用不同的镜像，生成容器# -it 开启一个交互式的终端–rm 容器退出时删除该容器 再运行一个7.8centos docker run -it –rm centos bash \\查看镜像** （1）查看所有镜像：docker images （2）指定tag查看：docker images centos:7.8.2003 （3）只列出镜像id #-q –quiet 只列出id：docker images -q （4）格式化显示镜像 这是docker的模板语言，–format{docker images –format “–“} \\删除镜像** （1）删除容器记录：docker rm 容器id （2）指定id的前三位即可：docker rmi 镜像id \\镜像管理** （1）批量删除镜像，慎用：docker rmi ‘docker images -aq’ （2）批量删除容器：docker rm ‘docker ps -aq’ （3）导出镜像：docker save -o nginx.tgz nginx:latest#打包tar包 （4）导入镜像： ①先删除本地的nginx镜像：docker rmi centos:7.8.2003 ②docker image load -i /export/software/centos1.8.2003.tgz#重新加载nginx-tar包 ③查看cocker服务的信息：docker info ④查看镜像详细信息：docker image inspact 镜像id \\docker镜像管理练习** （1）去DockerHub搜索Redies （2）利用docker pull命令拉去镜像：docker pull redis （3）查看Redies镜像的名称和版本：docker search redis （4）利用docker save命令将redies:latest打包为一个redies.tar包 docker save -o redis.tar redis:latest （5）利用docker rmi删除本地的redis:latest docker rmi redis:latest （6）利用docker load重新加载Redis.tar文件 docker load -i redis.tar \\容器操作** 创建并运行mn容器 docker run –name mn -p 80:80 -d nginx 运行刚刚创建的nginx容器 docker exec -it mn bash 进入nginx的HTML所在目录 /usr/share/nginx/html cd /usr/share/nginx/html 修改index.html的内容 sed -i -e ‘s#Welcome to nginx#人工智能学院欢迎您#g’ -e ‘s###g’ index.html \\创建和查看数据卷** 创建数据卷 docker volume create html 查看所有数据 docker volume ls 查看数据卷详细信息卷 docker volume inspect html 挂载数据卷 （1）创建容器并挂载数据卷到容器内的HTML目录，把/export/data/docker-data/nginx-html/数据卷挂载到容器内的/user/share/nginx/html目录中： docker run –name mn -v /export/data/docker-data/nginx-html/:/usr/share/nginx/html -p 80:80 -d nginx （2）进入html数据卷所在位置，并修改HTML内容 查看html数据卷的位置： docker volume inspect /export/data/docker-data/nginx-html/ （3）进入该目录 cd /export/data/docker-data/nginx-html/_data （4）修改文件vi index.html \\Docker应用部署** 搜索mysql镜像：docker search mysql 拉取mysql镜像 docker pull mysql:5.6 创建容器，设置端口映射、目录映射 mkdir -p /export/data/docker-data/mysql cd /export/data/docker-data/mysql docker run -id \\ -p 3306:3306 \\ –name=bigdata_mysql \\ -v $PWD/conf:/etc/mysql/conf.d \\ -v $PWD/logs:/logs \\ -v $PWD/data:/var/lib/mysql \\ -e MYSQL_ROOT_PASSWORD=hadoop \\ mysql:5.7.29 进入容器，操作mysql docker exec –it bigdata_mysql /bin/bash 使用外部机器连接容器中的mysql","tags":[{"name":"搭建","slug":"搭建","permalink":"http://example.com/tags/%E6%90%AD%E5%BB%BA/"},{"name":"spark","slug":"spark","permalink":"http://example.com/tags/spark/"}],"categories":[{"name":"工具","slug":"工具","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7/"}]},{"title":"Git安装","date":"2023-06-07T11:12:33.000Z","path":"wiki/Git安装/","text":"（1）Git下载 核心程序 （2）可视化客户端 中文语言包 （3）初始化仓库 （4）添加文件，提交文件至本地仓库 （5）本地删除与恢复 文件选中删除，可用以下方式还原 （6）创建分支 （7）分支的查看切换 （8）标签的创建 （9）切换与删除 通过右键选中删除 \\远程仓库** （1）码云账号注册 填写邮箱发送验证码,然后可以注册账号,主页如下 （2）创建远程仓库 （3）把本地代码推送到远端 生成公钥私钥 ssh-keygen -t rsa 一直回车 会默认用户目录 .ssh 目录生成一个默认的id_rsa文件 和id_rsa.pub 密钥配置","tags":[{"name":"搭建","slug":"搭建","permalink":"http://example.com/tags/%E6%90%AD%E5%BB%BA/"},{"name":"spark","slug":"spark","permalink":"http://example.com/tags/spark/"}],"categories":[{"name":"工具","slug":"工具","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7/"}]},{"title":"hive安装","date":"2023-06-07T10:36:00.000Z","path":"wiki/hive安装/","text":"\\（1）****Mysql安装**** 1）卸载Centos7自带的mariadb 如果出现了mariadb-libs-5.5.64-1.el7.x86_64，输入rpm -e mariadb- libs-5.5.64-1.el7.x86_64 –nodeps,在输入rpm -qa|grep mariadb，即可 2）安装mysql 新建文件夹：mkdir /export/server/mysql 上传mysql-5.7.29-1.el7.x86_64.rpm-bundle.tar到上述文件夹下，解压tar xvf mysql-5.7.29-1.el7.x86_64.rpm-bundle.tar 3）执行安装 yum -y install libaio 4）mysql初始化设置 初始化：mysqld –initialize 更改所属组：chown mysql:mysql /var/lib/mysql -R 启动mysql：systemctl start mysqld.service 查看临时生成的root密码：cat /var/log/mysqld.log 5）修改root密码 授权远程访问 设置开机自启动 修改root密码 设置为hadoop 授权 use mysql; GRANT ALL PRIVILEGES ON . TO ‘root‘@’%’ IDENTIFIED BY ‘hadoop’ WITH GRANT OPTION; FLUSH PRIVILEGES; mysql的启动和关闭 状态查看 （这几个命令必须记住） systemctl stop mysqld systemctl status mysqld systemctl start mysqld 设置开机自动启动：systemctl enable mysqld 查看是否设置自动启动成功 \\（2）****H****ive****的安装**** 1）上传安装包 解压 tar zxvf apache-hive-3.1.2-bin.tar.gz ln -s apache-hive-3.1.2-bin hive 2）解决Hive与Hadoop之间guava版本差异 cd /export/server/hive/ rm -rf lib/guava-19.0.jar cp /export/server/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar ./lib/ 3）修改配置文件 hive-env.sh cd /export/server/hive/conf mv hive-env.sh.template hive-env.sh vim hive-env.sh export HADOOP_HOME=/export/server/hadoop export HIVE_CONF_DIR=/export/server/hive/conf export HIVE_AUX_JARS_PATH=/export/server/hive/lib hive-site.xml vim hive-site.xml ​ javax.jdo.option.ConnectionURL​ jdbc:mysql://node1:3306/hive3?createDatabaseIfNotExist=true&amp;useSSL=false&amp;useUnicode=true&amp;characterEncoding=UTF-8​ javax.jdo.option.ConnectionDriverName​ com.mysql.jdbc.Driver​ javax.jdo.option.ConnectionUserName​ root​ javax.jdo.option.ConnectionPassword​ hadoop hive.server2.thrift.bind.host node1 hive.metastore.uris thrift://node1:9083 hive.metastore.event.db.notification.api.auth false 4）上传mysql jdbc驱动到hive安装包lib下 mysql-connector-java-5.1.32.jar 5）初始化元数据 cd /export/server/hive/ bin/schematool -initSchema -dbType mysql -verbos 初始化成功之后会在MySQL中创建74张表 6）在hdfs创建hive存储目录（如存在则不用操作） hadoop fs -mkdir /tmp hadoop fs -mkdir -p /user/hive/warehouse hadoop fs -chmod g+w /tmp hadoop fs -chmod g+w /user/hive/warehouse 7）启动hive \\（****3）****启动metastore服务**** 前台启动 关闭ctrl+c /export/server/hive/bin/hive –service metastore 前台启动开启debug日志 /export/server/hive/bin/hive –service metastore –hiveconf hive.root.logger=DEBUG,console 后台启动 进程挂起 关闭使用jps+ kill -9 nohup /export/server/hive/bin/hive –service metastore &amp; \\（****4）****启动hiveserver2服务**** nohup /export/server/hive/bin/hive –service hiveserver2 &amp; beeline客户端连接 拷贝node1安装包到beeline客户端机器上（node3） scp -r /export/server/apache-hive-3.1.2-bin/ root@node3:/export/server/ \\（5）****hive注释信息中文乱码解决**** 以下sql语句均在mysql数据库中执行 use hivenode2; show tables; alter table hivenode2.COLUMNS_V2 modify column COMMENT varchar(256) character set utf8; alter table hivenode2.TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8; alter table hivenode2.PARTITION_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8 ; alter table hivenode2.PARTITION_KEYS modify column PKEY_COMMENT varchar(4000) character set utf8; alter table hivenode2.INDEX_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;","tags":[{"name":"搭建","slug":"搭建","permalink":"http://example.com/tags/%E6%90%AD%E5%BB%BA/"},{"name":"spark","slug":"spark","permalink":"http://example.com/tags/spark/"}],"categories":[{"name":"工具","slug":"工具","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7/"}]}],"categories":[{"name":"工具","slug":"工具","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"搭建","slug":"搭建","permalink":"http://example.com/tags/%E6%90%AD%E5%BB%BA/"},{"name":"spark","slug":"spark","permalink":"http://example.com/tags/spark/"}]}